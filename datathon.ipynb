{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmHykLagB2e7"
      },
      "source": [
        "# Datathon 2025 RAG Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUr8ufSfHSHM"
      },
      "source": [
        "### Ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDHC0aF4ZTzD"
      },
      "source": [
        "Let's first import relevant packages, take a look at our data, and create a master dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mVZ5V18dHOGr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jIn0f0piCGkJ"
      },
      "outputs": [],
      "source": [
        "media_df = pd.read_csv('Datasets/media.csv')\n",
        "places_p2_df = pd.read_csv('Datasets/places.csv')\n",
        "reviews_df = pd.read_csv('Datasets/reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sqs4G-oSNhSg",
        "outputId": "3c002e37-9991-4e14-fe47-09d8be7e32ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "P2 Media preview: <bound method NDFrame.head of          place_id                                          media_url\n",
            "0         place_1  https://cdn.corner.inc/place-photo/AUjq9jnss_x...\n",
            "1         place_1  https://cdn.corner.inc/place-photo/AUjq9jliO8l...\n",
            "2         place_1  https://cdn.corner.inc/place-photo/AUjq9jmYn9S...\n",
            "3         place_1  https://cdn.corner.inc/place-photo/AUjq9jnc5Zm...\n",
            "4         place_1  https://cdn.corner.inc/place-photo/AUjq9jmiIE3...\n",
            "...           ...                                                ...\n",
            "37617  place_1500  https://cdn.corner.inc/place-photo/AUGGfZkzoPe...\n",
            "37618  place_1500  https://cdn.corner.inc/place-photo/AUGGfZniHDf...\n",
            "37619  place_1500  https://cdn.corner.inc/place-photo/AUGGfZnZCnm...\n",
            "37620  place_1500  https://cdn.corner.inc/place-photo/AUGGfZn2wUa...\n",
            "37621  place_1500  https://cdn.corner.inc/ugc/9781d311-2298-4582-...\n",
            "\n",
            "[37622 rows x 2 columns]>\n",
            "\n",
            "P2 Places preview: <bound method NDFrame.head of         place_id                        name  neighborhood  latitude  \\\n",
            "0        place_1              Public Records      Brooklyn  40.68227   \n",
            "1        place_2              Silence Please           NaN  40.71895   \n",
            "2        place_3                    schmuck.           NaN  40.72637   \n",
            "3        place_4                  The Django       Tribeca  40.71941   \n",
            "4        place_5      Honeycomb Hi-Fi Lounge    Park Slope  40.68077   \n",
            "...          ...                         ...           ...       ...   \n",
            "1495  place_1496                 Maki Kosaka       Chelsea  40.74029   \n",
            "1496  place_1497  Goods for the Study Nolita  Little Italy  40.72264   \n",
            "1497  place_1498                  Little Egg      Brooklyn  40.67769   \n",
            "1498  place_1499               Textbook Cafe   Fort Greene  40.68970   \n",
            "1499  place_1500                   Frame NYC    Lenox Hill  40.76999   \n",
            "\n",
            "      longitude                              tags    short_description emoji  \n",
            "0     -73.98640  {night_club,cafe,bar,restaurant}     vinyl dance club     üíø  \n",
            "1     -73.99490                            {cafe}           vinyl cafe     üíø  \n",
            "2     -73.98647                             {bar}      craft cocktails     üç∏  \n",
            "3     -74.00491       {bar,night_club,restaurant}     underground jazz     üé∑  \n",
            "4     -73.97775                             {bar}        listening bar     üéµ  \n",
            "...         ...                               ...                  ...   ...  \n",
            "1495  -73.99382                      {restaurant}       hidden omakase     üç±  \n",
            "1496  -73.99590                            {shop}   stationery & gifts     ‚úâ  \n",
            "1497  -73.96352                      {restaurant}             egg cafe     üç≥  \n",
            "1498  -73.97775                 {restaurant,cafe}  cafe with free wine     üç∑  \n",
            "1499  -73.95773                            {cafe}      coffee & brunch     ‚òï  \n",
            "\n",
            "[1500 rows x 8 columns]>\n",
            "\n",
            "P2 Reviews preview: <bound method NDFrame.head of          place_id                                        review_text\n",
            "0         place_1  the best place to dance until 4am in nyc. get ...\n",
            "1         place_1                      hot. dance. floor. make outs.\n",
            "2         place_1  The oontz oontz dance floor is thumpin, the ba...\n",
            "3         place_1                             good for day and night\n",
            "4         place_1                       third floor is where it‚Äôs at\n",
            "...           ...                                                ...\n",
            "15238  place_1500  I cannot say enough good things about this pla...\n",
            "15239  place_1500  Great food. The avocado toast was some of the ...\n",
            "15240  place_1500  Great atmosphere - super beautiful and Instagr...\n",
            "15241  place_1500  cozy as hell. outlets for days. coffee and mat...\n",
            "15242  place_1500  great. (+) wifi, public restroom, some outlets...\n",
            "\n",
            "[15243 rows x 2 columns]>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"P2 Media preview: {media_df.head}\\n\")\n",
        "print(f\"P2 Places preview: {places_p2_df.head}\\n\")\n",
        "print(f\"P2 Reviews preview: {reviews_df.head}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbV5aAv6EVPR"
      },
      "source": [
        "Aggregate reviews corresponding to a given place_id into a list of strings. Do the same for media, then merge both into larger df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "jlvKugyQNY_d",
        "outputId": "750829d4-750d-4662-a484-1320e4e8a711"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>place_id</th>\n",
              "      <th>name</th>\n",
              "      <th>neighborhood</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>tags</th>\n",
              "      <th>short_description</th>\n",
              "      <th>emoji</th>\n",
              "      <th>media_url</th>\n",
              "      <th>review_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>place_1</td>\n",
              "      <td>Public Records</td>\n",
              "      <td>Brooklyn</td>\n",
              "      <td>40.68227</td>\n",
              "      <td>-73.98640</td>\n",
              "      <td>{night_club,cafe,bar,restaurant}</td>\n",
              "      <td>vinyl dance club</td>\n",
              "      <td>üíø</td>\n",
              "      <td>[https://cdn.corner.inc/place-photo/AUjq9jnss_...</td>\n",
              "      <td>[the best place to dance until 4am in nyc. get...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>place_2</td>\n",
              "      <td>Silence Please</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.71895</td>\n",
              "      <td>-73.99490</td>\n",
              "      <td>{cafe}</td>\n",
              "      <td>vinyl cafe</td>\n",
              "      <td>üíø</td>\n",
              "      <td>[https://cdn.corner.inc/place-photo/AWYs27xW6j...</td>\n",
              "      <td>[i heard they charge an entrance fee now at th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>place_3</td>\n",
              "      <td>schmuck.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>40.72637</td>\n",
              "      <td>-73.98647</td>\n",
              "      <td>{bar}</td>\n",
              "      <td>craft cocktails</td>\n",
              "      <td>üç∏</td>\n",
              "      <td>[https://cdn.corner.inc/ugc/0875b9e6-d6fe-4db1...</td>\n",
              "      <td>[apparently this is very vibey and THE spot bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>place_4</td>\n",
              "      <td>The Django</td>\n",
              "      <td>Tribeca</td>\n",
              "      <td>40.71941</td>\n",
              "      <td>-74.00491</td>\n",
              "      <td>{bar,night_club,restaurant}</td>\n",
              "      <td>underground jazz</td>\n",
              "      <td>üé∑</td>\n",
              "      <td>[https://cdn.corner.inc/place-photo/AUjq9jkq_2...</td>\n",
              "      <td>[The prettiest jazz club I‚Äôve been! Good cockt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>place_5</td>\n",
              "      <td>Honeycomb Hi-Fi Lounge</td>\n",
              "      <td>Park Slope</td>\n",
              "      <td>40.68077</td>\n",
              "      <td>-73.97775</td>\n",
              "      <td>{bar}</td>\n",
              "      <td>listening bar</td>\n",
              "      <td>üéµ</td>\n",
              "      <td>[https://cdn.corner.inc/place-photo/cb4ddc19-d...</td>\n",
              "      <td>[listening bar, One of my favorite bars in NYC...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  place_id                    name neighborhood  latitude  longitude  \\\n",
              "0  place_1          Public Records     Brooklyn  40.68227  -73.98640   \n",
              "1  place_2          Silence Please          NaN  40.71895  -73.99490   \n",
              "2  place_3                schmuck.          NaN  40.72637  -73.98647   \n",
              "3  place_4              The Django      Tribeca  40.71941  -74.00491   \n",
              "4  place_5  Honeycomb Hi-Fi Lounge   Park Slope  40.68077  -73.97775   \n",
              "\n",
              "                               tags short_description emoji  \\\n",
              "0  {night_club,cafe,bar,restaurant}  vinyl dance club     üíø   \n",
              "1                            {cafe}        vinyl cafe     üíø   \n",
              "2                             {bar}   craft cocktails     üç∏   \n",
              "3       {bar,night_club,restaurant}  underground jazz     üé∑   \n",
              "4                             {bar}     listening bar     üéµ   \n",
              "\n",
              "                                           media_url  \\\n",
              "0  [https://cdn.corner.inc/place-photo/AUjq9jnss_...   \n",
              "1  [https://cdn.corner.inc/place-photo/AWYs27xW6j...   \n",
              "2  [https://cdn.corner.inc/ugc/0875b9e6-d6fe-4db1...   \n",
              "3  [https://cdn.corner.inc/place-photo/AUjq9jkq_2...   \n",
              "4  [https://cdn.corner.inc/place-photo/cb4ddc19-d...   \n",
              "\n",
              "                                         review_text  \n",
              "0  [the best place to dance until 4am in nyc. get...  \n",
              "1  [i heard they charge an entrance fee now at th...  \n",
              "2  [apparently this is very vibey and THE spot bu...  \n",
              "3  [The prettiest jazz club I‚Äôve been! Good cockt...  \n",
              "4  [listening bar, One of my favorite bars in NYC...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reviews_df = reviews_df.drop_duplicates(subset=['place_id', 'review_text'])\n",
        "media_df = media_df.drop_duplicates(subset=['place_id', 'media_url'])\n",
        "\n",
        "reviews_agg = (\n",
        "    reviews_df\n",
        "    .groupby('place_id')['review_text']\n",
        "    .apply(list)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "media_agg = (\n",
        "    media_df\n",
        "    .groupby('place_id')['media_url']\n",
        "    .apply(list)\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "merge_df = pd.merge(places_p2_df, media_agg, on='place_id', how='inner')\n",
        "merge_df = pd.merge(merge_df, reviews_agg, on='place_id', how='inner')\n",
        "merge_df = merge_df.drop_duplicates(subset='place_id').reset_index(drop=True)\n",
        "merge_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABoPHFQiQhOM"
      },
      "source": [
        "Let's concatenate reviews associated with a given place_id in preparation for embedding. Hopefully this can help with vibier searches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xzwHTgJpRPQN"
      },
      "outputs": [],
      "source": [
        "def concat_reviews(series):\n",
        "    \"\"\"Join all review texts in the group into a single string.\"\"\"\n",
        "    return ' '.join(series.astype(str))\n",
        "\n",
        "# Group reviews and create the all_reviews column\n",
        "agg_reviews = (\n",
        "    reviews_df\n",
        "    .groupby('place_id')['review_text']\n",
        "    .apply(concat_reviews)\n",
        "    .reset_index(name='concat_reviews')\n",
        ")\n",
        "\n",
        "# Merge the concatenated reviews into merge_df\n",
        "merge_df = merge_df.merge(\n",
        "    agg_reviews,\n",
        "    how='left',\n",
        "    left_on='place_id',\n",
        "    right_on='place_id'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXGgzRbQGRUn"
      },
      "source": [
        "Let's write a function to prepare available structured data for semantic embedding, tack on the concatenated reviews and add it to our df."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Gy5YgKDTLm6F"
      },
      "outputs": [],
      "source": [
        "def combining_text(row):\n",
        "    name = str(row.get('name', ''))\n",
        "    neighborhood = str(row.get('neighborhood', ''))\n",
        "    tags = str(row.get('tags', ''))\n",
        "    short_description = str(row.get('short_description', ''))\n",
        "    emojis = str(row.get('emojis', ''))\n",
        "    reviews = str(row.get('concat_reviews', ''))\n",
        "\n",
        "    combined_text = (\n",
        "        f\"Name: {name}. \"\n",
        "        f\"Neighborhood: {neighborhood}. \"\n",
        "        f\"Tags: {tags}. \"\n",
        "        f\"Description: {short_description}. \"\n",
        "        f\"Emojis: {emojis}.\"\n",
        "        f\"User Reviews: {reviews}.\"\n",
        "    )\n",
        "    return combined_text\n",
        "\n",
        "merge_df['combined_text'] = merge_df.apply(combining_text, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b13bfa8cQVcL"
      },
      "source": [
        "Ok, now let's actually start embedding our data. Cross-Encoders will be too computationally heavy for this. Instead will use bi-encoders such as sentence_transformers and CLIP for seemantic and multimodal embeddings respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQzwCV35KnAD"
      },
      "source": [
        "### Metadata Text Embeddings (Dense + Sparse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FipmpqyqafSN"
      },
      "source": [
        "Eventually, we want a hybrid search which requires is a combination of dense, sparse, and multimodal embeddings. Let's start with dense."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/tomasgutierrez/projects/personal/corner-datathon/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Load tqdm and MiniLM model(dense semantic text embedding)\n",
        "from tqdm.auto import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "metadata_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating embeddings from dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [00:20<00:00,  2.33it/s]\n"
          ]
        }
      ],
      "source": [
        "embeddings = []\n",
        "batch_size = 32\n",
        "\n",
        "for i in tqdm(range(0, len(merge_df), batch_size), desc=\"Generating embeddings from dataset\"):\n",
        "    batch = merge_df['combined_text'].iloc[i: i + batch_size].tolist()\n",
        "    batch_embeddings = metadata_model.encode(batch, normalize_embeddings=True)\n",
        "    embeddings.append(batch_embeddings)\n",
        "\n",
        "embeddings = np.vstack(embeddings)\n",
        "\n",
        "# Save in Dataframe\n",
        "merge_df['dense_metadata_embedding'] = embeddings.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4GB6MBidMtN"
      },
      "source": [
        "Ok, now let's try a sparse text embedding model from fastembed. Hopefully with the hybrid, we can pick up explicit meaning as well as implied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 5 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:09<00:00,  1.96s/it]\n"
          ]
        }
      ],
      "source": [
        "from fastembed import SparseTextEmbedding\n",
        "sparse_model = SparseTextEmbedding(model_name=\"prithivida/Splade_PP_en_v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fRLY4gFgz2K"
      },
      "source": [
        "Time to embed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating sparse embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 47/47 [03:36<00:00,  4.60s/it]\n"
          ]
        }
      ],
      "source": [
        "sparse_embeddings = []\n",
        "\n",
        "for i in tqdm(range(0, len(merge_df), batch_size), desc=\"Generating sparse embeddings\"):\n",
        "    batch = merge_df['combined_text'].iloc[i: i + batch_size].tolist()\n",
        "    batch_embeddings = list(sparse_model.embed(batch))\n",
        "    sparse_embeddings.extend(batch_embeddings)\n",
        "\n",
        "merge_df['sparse_metadata_embedding'] = sparse_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqjfKikdnqgx"
      },
      "source": [
        "Let's save it real quick."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_uHvMALgnnu5"
      },
      "outputs": [],
      "source": [
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDUTeANUnk4c",
        "outputId": "a75d467c-0883-44b0-f91d-772c56d38160"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['merge_dense_and_sparse_df.joblib']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the DataFrame\n",
        "joblib.dump(merge_df, 'merge_dense_and_sparse_df.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyhM1mk-HbJy"
      },
      "source": [
        "### Media (Image) Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0AD7aIn9OfL"
      },
      "source": [
        "Because there are over 30,000 media_urls, We opted for batch processing 1 image per place_id, reducing the image's resolution, and then embedding it using a Hugging Face CLIP model for multimodal embedding in another notebook. The result is the image_embeddings_sorted.csv that we can simply read in as a df. Ideally, with more time we could embed several/all images corresponding to a place_id, and then take their arithmetic mean for a more generally representative embedding per place_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4qR6h-y2KVXp"
      },
      "outputs": [],
      "source": [
        "image_df = pd.read_csv('Datasets/image_embeddings_sorted.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnn7DfaGKmxb"
      },
      "source": [
        "Also, even though we already embedded the images in another file, let's import and load the CLIP model hear to use for embedding queries in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "# Embedded 1 image for each location using CLIPProcessor, CLIPModel in another ipynb\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DMYDVBO4gk4"
      },
      "source": [
        "### Vector Similarity Search + Hybrid Search Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frKcyrazC1nm"
      },
      "source": [
        "Let's use Facebook AI Similarity Search (FAISS) for efficient vector similarity search on our:\n",
        "- Metadata Embeddings (Dense)\n",
        "- Media Embeddings (Image)\n",
        "\n",
        "And, let's use Cosine Similarity for our sparse model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mTcFjYd5MOPq"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UbN_UK4IE6T"
      },
      "source": [
        "Metadata FAISS (Dense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfYcQivnIKaI",
        "outputId": "5274799c-4e49-4077-95e9-639477f2aad0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built FAISS dense metadata index with 1499 vectors of dimension 384.\n"
          ]
        }
      ],
      "source": [
        "# Get metadata embeddings\n",
        "all_dense_embeddings = np.array(merge_df['dense_metadata_embedding'].tolist(), dtype='float32')\n",
        "\n",
        "# Determine embedding dimension\n",
        "d = all_dense_embeddings.shape[1]\n",
        "\n",
        "# Create FAISS index (L2 distance)\n",
        "index_dense_metadata = faiss.IndexFlatL2(d)\n",
        "index_dense_metadata.add(all_dense_embeddings)\n",
        "\n",
        "print(f\"Built FAISS dense metadata index with {index_dense_metadata.ntotal} vectors of dimension {d}.\")\n",
        "\n",
        "# Let's make a search function\n",
        "def search_places_dense_metadata(query, index=index_dense_metadata, top_k=5):\n",
        "    query_embedding = metadata_model.encode([query], normalize_embeddings=True)[0].astype('float32').reshape(1, -1)\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        row = merge_df.iloc[idx]\n",
        "        results.append({\n",
        "            'Place Name': row['name'],\n",
        "            'Neighborhood': row['neighborhood'],\n",
        "            'Tags': row['tags'],\n",
        "            'Description': row['short_description'],\n",
        "            'Distance': distances[0][i]\n",
        "        })\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_41hx_N7L51",
        "outputId": "eaddb47a-38bf-4a48-fde9-36f3c2b8b4d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Place Name   : Sorate\n",
            "Neighborhood : SoHo\n",
            "Tags         : {destination}\n",
            "Description  : matcha bar\n",
            "L2 Distance  : 0.7257\n",
            "------------------------------\n",
            "Place Name   : The Mandarin\n",
            "Neighborhood : nan\n",
            "Tags         : {cafe}\n",
            "Description  : cozy cafe\n",
            "L2 Distance  : 0.7298\n",
            "------------------------------\n",
            "Place Name   : Setsugekka East Village\n",
            "Neighborhood : East Village\n",
            "Tags         : {shop}\n",
            "Description  : japanese tea house\n",
            "L2 Distance  : 0.7679\n",
            "------------------------------\n",
            "Place Name   : Kettl Tea\n",
            "Neighborhood : NoHo\n",
            "Tags         : {destination}\n",
            "Description  : matcha & soba tea\n",
            "L2 Distance  : 0.7777\n",
            "------------------------------\n",
            "Place Name   : Nana‚Äôs Green Tea\n",
            "Neighborhood : Koreatown\n",
            "Tags         : {restaurant}\n",
            "Description  : japanese desserts\n",
            "L2 Distance  : 0.8102\n"
          ]
        }
      ],
      "source": [
        "# Let's try a query and output the 5 nearest embeddings\n",
        "query = \"where to drink a matcha\"\n",
        "search_results = search_places_dense_metadata(query, index_dense_metadata, top_k=5)\n",
        "\n",
        "for result in search_results:\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Place Name   : {result['Place Name']}\")\n",
        "    print(f\"Neighborhood : {result['Neighborhood']}\")\n",
        "    print(f\"Tags         : {result['Tags']}\")\n",
        "    print(f\"Description  : {result['Description']}\")\n",
        "    print(f\"L2 Distance  : {result['Distance']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUpUuDDWN-EH"
      },
      "source": [
        "Image FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OsT-8TQ4cMe",
        "outputId": "26e0ba5a-c792-42e2-d545-2bf9242361a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built FAISS image index with 1499 vectors of dimension 512.\n"
          ]
        }
      ],
      "source": [
        "# Extract embedding columns (assumes embed_0 to embed_511)\n",
        "embedding_cols = [col for col in image_df.columns if col.startswith('embed_')]\n",
        "all_embeddings = image_df[embedding_cols].to_numpy().astype('float32')\n",
        "\n",
        "# Determine embedding dimension\n",
        "d = all_embeddings.shape[1]\n",
        "\n",
        "# Build FAISS index\n",
        "index_image = faiss.IndexFlatL2(d)\n",
        "index_image.add(all_embeddings)\n",
        "\n",
        "print(f\"Built FAISS image index with {index_image.ntotal} vectors of dimension {d}.\")\n",
        "\n",
        "# Let's make a search function\n",
        "def search_places_image(query, index=index_image, top_k=5):\n",
        "    # Encode the text query using CLIP text encoder\n",
        "    inputs = processor(text=[query], return_tensors=\"pt\", padding=True)\n",
        "    query_embedding = clip_model.get_text_features(**inputs)\n",
        "    query_embedding = query_embedding / query_embedding.norm(p=2, dim=-1, keepdim=True)\n",
        "    query_embedding = query_embedding.detach().cpu().numpy().astype('float32').reshape(1, -1)\n",
        "\n",
        "    # Perform FAISS search\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    # Gather results from image_df (which holds the place info)\n",
        "    results = []\n",
        "    for i, idx in enumerate(indices[0]):\n",
        "        row = merge_df.iloc[idx]\n",
        "        results.append({\n",
        "            'Place Name': row['name'],\n",
        "            'Neighborhood': row['neighborhood'],\n",
        "            'Tags': row['tags'],\n",
        "            'Description': row['short_description'],\n",
        "            'Distance': distances[0][i]\n",
        "        })\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge6u7_Ae5P-O",
        "outputId": "12c7c70e-f7e4-41a2-8570-7ef6f835daf0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Place Name   : Bird & Branch Coffee Roasters\n",
            "Neighborhood : Hell's Kitchen\n",
            "Tags         : {cafe}\n",
            "Description  : specialty coffee\n",
            "L2 Distance  : 1.5070\n",
            "------------------------------\n",
            "Place Name   : ThirdSpace\n",
            "Neighborhood : Brooklyn\n",
            "Tags         : {destination}\n",
            "Description  : creative events\n",
            "L2 Distance  : 1.5274\n",
            "------------------------------\n",
            "Place Name   : % Arabica New York Nolita\n",
            "Neighborhood : Little Italy\n",
            "Tags         : {cafe}\n",
            "Description  : coffee & pastries\n",
            "L2 Distance  : 1.5314\n",
            "------------------------------\n",
            "Place Name   : Supermoon Bakehouse\n",
            "Neighborhood : Lower East Side\n",
            "Tags         : {bakery}\n",
            "Description  : creative bakery\n",
            "L2 Distance  : 1.5317\n",
            "------------------------------\n",
            "Place Name   : Little Ruby's East Village\n",
            "Neighborhood : East Village\n",
            "Tags         : {restaurant}\n",
            "Description  : aussie brunch\n",
            "L2 Distance  : 1.5338\n"
          ]
        }
      ],
      "source": [
        "# Let's try a query and output the 5 nearest embeddings\n",
        "search_results = search_places_image(\"Something to do on a gloomy day\", index_image, top_k=5)\n",
        "\n",
        "for result in search_results:\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Place Name   : {result['Place Name']}\")\n",
        "    print(f\"Neighborhood : {result['Neighborhood']}\")\n",
        "    print(f\"Tags         : {result['Tags']}\")\n",
        "    print(f\"Description  : {result['Description']}\")\n",
        "    print(f\"L2 Distance  : {result['Distance']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtnFE_0Ssdur"
      },
      "source": [
        "Metadata Cosine Similarity (Sparse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting sparse embeddings to dense matrix...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Converting embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1499/1499 [00:00<00:00, 26082.70it/s]\n"
          ]
        }
      ],
      "source": [
        "def sparse_to_dense(sparse_embedding, dim=30315):\n",
        "    dense_vec = np.zeros(dim, dtype=np.float32)\n",
        "    dense_vec[sparse_embedding.indices] = sparse_embedding.values\n",
        "    return dense_vec\n",
        "\n",
        "print(\"Converting sparse embeddings to dense matrix...\")\n",
        "sparse_embeddings_dense = np.vstack([\n",
        "    sparse_to_dense(embedding, dim=30315)\n",
        "    for embedding in tqdm(merge_df['sparse_metadata_embedding'], desc=\"Converting embeddings\")\n",
        "])\n",
        "\n",
        "# Let's make a search function\n",
        "def search_places_sparse_metadata(query, sparse_model, embeddings_matrix, top_k=5):\n",
        "    # Embed the query (FastEmbed sparse model)\n",
        "    query_sparse = list(sparse_model.embed([query]))[0]\n",
        "    query_dense = sparse_to_dense(query_sparse, dim=30315).reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarities = cosine_similarity(query_dense, embeddings_matrix)[0]\n",
        "\n",
        "    # Fast top-k retrieval\n",
        "    top_indices = np.argpartition(-similarities, top_k)[:top_k]\n",
        "    top_indices = top_indices[np.argsort(similarities[top_indices])[::-1]]\n",
        "\n",
        "    # Prepare results\n",
        "    results = []\n",
        "    for idx in top_indices:\n",
        "        row = merge_df.iloc[idx]\n",
        "        results.append({\n",
        "            'Place Name': row['name'],\n",
        "            'Neighborhood': row['neighborhood'],\n",
        "            'Tags': row['tags'],\n",
        "            'Description': row['short_description'],\n",
        "            'Similarity': similarities[idx]\n",
        "        })\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQJL63Omv0XM",
        "outputId": "aded4e0b-72de-459a-ce55-d7b06cc570ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "Place Name        : Cafe Balearica\n",
            "Neighborhood      : Brooklyn\n",
            "Tags              : {bar,restaurant}\n",
            "Description       : disco bar\n",
            "Cosine Similarity : 0.3478\n",
            "------------------------------\n",
            "Place Name        : Ciao Ciao Disco\n",
            "Neighborhood      : Brooklyn\n",
            "Tags              : {bar}\n",
            "Description       : disco drag club\n",
            "Cosine Similarity : 0.3199\n",
            "------------------------------\n",
            "Place Name        : Gabriela\n",
            "Neighborhood      : Brooklyn\n",
            "Tags              : {bar}\n",
            "Description       : disco club\n",
            "Cosine Similarity : 0.2824\n",
            "------------------------------\n",
            "Place Name        : Jupiter Disco\n",
            "Neighborhood      : Bushwick\n",
            "Tags              : {bar,night_club}\n",
            "Description       : space disco\n",
            "Cosine Similarity : 0.2816\n",
            "------------------------------\n",
            "Place Name        : Doris\n",
            "Neighborhood      : Brooklyn\n",
            "Tags              : {bar}\n",
            "Description       : disco cocktails\n",
            "Cosine Similarity : 0.2725\n"
          ]
        }
      ],
      "source": [
        "# Let's try a query and output the 5 nearest embeddings\n",
        "search_results = search_places_sparse_metadata(\"dance-y bars that have disco balls\", sparse_model, sparse_embeddings_dense, top_k=5)\n",
        "\n",
        "# Output the results\n",
        "for result in search_results:\n",
        "    print(\"------------------------------\")\n",
        "    print(f\"Place Name        : {result['Place Name']}\")\n",
        "    print(f\"Neighborhood      : {result['Neighborhood']}\")\n",
        "    print(f\"Tags              : {result['Tags']}\")\n",
        "    print(f\"Description       : {result['Description']}\")\n",
        "    print(f\"Cosine Similarity : {result['Similarity']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4eh2osw1k5L"
      },
      "source": [
        "Now that we have our three similarity search functions, let's final make our hybrid search function! First, we need to normalize FAISS distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "td5HgszM41ou"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ozwtPt_W2mOt"
      },
      "outputs": [],
      "source": [
        "# helper to normalize scores between 0 and 1\n",
        "def normalize_scores(scores):\n",
        "    scores = np.array(scores).reshape(-1, 1)\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(scores).flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7tLx5fndBWtr"
      },
      "outputs": [],
      "source": [
        "# hybrid search function\n",
        "# Full hybrid search function\n",
        "def hybrid_search(query, metadata_index, image_index, sparse_embeddings,\n",
        "                  sparse_model, metadata_model, processor, clip_model, top_k=5,\n",
        "                  weight_dense=0.4, weight_sparse=0.3, weight_image=0.3):\n",
        "    # Metadata (Dense)\n",
        "    query_dense = metadata_model.encode([query], normalize_embeddings=True)[0].astype('float32').reshape(1, -1)\n",
        "    distances_dense, indices_dense = metadata_index.search(query_dense, top_k)\n",
        "    scores_dense = -distances_dense[0]  # Negative L2 distance ‚Üí higher is better\n",
        "\n",
        "    # Sparse Embeddings\n",
        "    query_sparse = list(sparse_model.embed([query]))[0]\n",
        "    query_sparse_dense = sparse_to_dense(query_sparse, dim=30315).reshape(1, -1)\n",
        "    similarities_sparse = cosine_similarity(query_sparse_dense, sparse_embeddings)[0]\n",
        "    top_indices_sparse = np.argpartition(-similarities_sparse, top_k)[:top_k]\n",
        "    top_indices_sparse = top_indices_sparse[np.argsort(similarities_sparse[top_indices_sparse])[::-1]]\n",
        "    scores_sparse = similarities_sparse[top_indices_sparse]\n",
        "\n",
        "    # Image Embeddings\n",
        "    inputs = processor(text=[query], return_tensors=\"pt\", padding=True)\n",
        "    query_image_embedding = clip_model.get_text_features(**inputs)\n",
        "    query_image_embedding = query_image_embedding / query_image_embedding.norm(p=2, dim=-1, keepdim=True)\n",
        "    query_image_embedding = query_image_embedding.detach().cpu().numpy().astype('float32').reshape(1, -1)\n",
        "    distances_image, indices_image = image_index.search(query_image_embedding, top_k)\n",
        "    scores_image = -distances_image[0]  # Negative L2 distance ‚Üí higher is better\n",
        "\n",
        "    # Normalize Scores\n",
        "    norm_dense = normalize_scores(scores_dense)\n",
        "    norm_sparse = normalize_scores(scores_sparse)\n",
        "    norm_image = normalize_scores(scores_image)\n",
        "\n",
        "    # Hybrid Scoring\n",
        "    hybrid_scores = (weight_dense * norm_dense[:top_k] +\n",
        "                     weight_sparse * norm_sparse[:top_k] +\n",
        "                     weight_image * norm_image[:top_k])\n",
        "\n",
        "    # Gather Results\n",
        "    results = []\n",
        "    for i in range(top_k):\n",
        "        idx = indices_dense[0][i]  # Take from dense indices\n",
        "        row = merge_df.iloc[idx]\n",
        "        results.append({\n",
        "            'Place Name': row['name'],\n",
        "            'Neighborhood': row['neighborhood'],\n",
        "            'Tags': row['tags'],\n",
        "            'Description': row['short_description'],\n",
        "            'Hybrid Score': hybrid_scores[i],\n",
        "            'Dense Score': norm_dense[i],\n",
        "            'Sparse Score': norm_sparse[i],\n",
        "            'Image Score': norm_image[i]\n",
        "        })\n",
        "\n",
        "    results = [res for res in results if res['Hybrid Score'] > 0.1]\n",
        "\n",
        "    # Sort Final Output\n",
        "    results = sorted(results, key=lambda x: x['Hybrid Score'], reverse=True)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BIQZ_vDJCSVP"
      },
      "source": [
        "Let's test out our hybrid_search function - It should be a great at specific and vague queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3OobQG2287A",
        "outputId": "48b0eb9b-7621-425f-97d1-e67501523c7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------\n",
            "Place Name       : VITAL Climbing Gym - Brooklyn\n",
            "Neighborhood     : Brooklyn\n",
            "Tags             : {health}\n",
            "Description      : rooftop bouldering\n",
            "Hybrid Score     : 1.0000\n",
            "Dense Score      : 1.0000\n",
            "Image Score      : 1.0000\n",
            "Sparse Score     : 1.0000\n",
            "-------------------------\n",
            "Place Name       : Pebble Beach\n",
            "Neighborhood     : Brooklyn\n",
            "Tags             : {nature}\n",
            "Description      : riverside views\n",
            "Hybrid Score     : 0.3371\n",
            "Dense Score      : 0.4505\n",
            "Image Score      : 0.1954\n",
            "Sparse Score     : 0.3276\n",
            "-------------------------\n",
            "Place Name       : Cafe Mogador\n",
            "Neighborhood     : East Village\n",
            "Tags             : {bar,restaurant}\n",
            "Description      : moroccan cafe\n",
            "Hybrid Score     : 0.2378\n",
            "Dense Score      : 0.4153\n",
            "Image Score      : 0.1779\n",
            "Sparse Score     : 0.0612\n"
          ]
        }
      ],
      "source": [
        "user_query = \"what to if I want to exercise\"\n",
        "\n",
        "results = hybrid_search(\n",
        "    query=user_query,\n",
        "    metadata_index=index_dense_metadata,       # dense FAISS index\n",
        "    image_index=index_image,                   # image FAISS index\n",
        "    sparse_embeddings=sparse_embeddings_dense, # dense-converted sparse embeddings\n",
        "    metadata_model=metadata_model,\n",
        "    sparse_model=sparse_model,                 # Dense text embedding model (MiniLM)\n",
        "    processor=processor,                       # CLIP processor\n",
        "    clip_model=clip_model,                         # CLIP model\n",
        ")\n",
        "\n",
        "for res in results:\n",
        "    print(\"-------------------------\")\n",
        "    print(f\"Place Name       : {res['Place Name']}\")\n",
        "    print(f\"Neighborhood     : {res['Neighborhood']}\")\n",
        "    print(f\"Tags             : {res['Tags']}\")\n",
        "    print(f\"Description      : {res['Description']}\")\n",
        "    print(f\"Hybrid Score     : {res['Hybrid Score']:.4f}\")\n",
        "    print(f\"Dense Score      : {res['Dense Score']:.4f}\")\n",
        "    print(f\"Image Score      : {res['Image Score']:.4f}\")\n",
        "    print(f\"Sparse Score     : {res['Sparse Score']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSgNJIGhHH9Z",
        "outputId": "66678dc8-138c-43bd-d998-2e19d7ea4888"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged.csv with all embeddings.\n",
            "Saved metadata.index\n",
            "Saved image.index\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# 1. Save merged dataframe with all embeddings\n",
        "output_dir = \"Datasets/processed_data\"\n",
        "Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "merge_df.to_csv(f\"{output_dir}/merged.csv\", index=False)\n",
        "print(\"Saved merged.csv with all embeddings.\")\n",
        "\n",
        "# 2. Save dense metadata FAISS index\n",
        "faiss.write_index(index_dense_metadata, f\"{output_dir}/metadata.index\")\n",
        "print(\"Saved metadata.index\")\n",
        "\n",
        "# 3. Save image embedding FAISS index\n",
        "faiss.write_index(index_image, f\"{output_dir}/image.index\")\n",
        "print(\"Saved image.index\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
